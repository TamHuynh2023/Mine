{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "6GOdbk8XKuzo"
      },
      "outputs": [],
      "source": [
        "!pip cache purge\n",
        "\n",
        "!pip install python-dotenv\n",
        "!pip install httpx==0.27.2\n",
        "!pip install anthropic\n",
        "!pip install gradio\n",
        "!pip install torch\n",
        "!pip install -q transformers datasets diffusers\n",
        "!pip install -q requests bitsandbytes accelerate sentencepiece\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "import torchvision\n",
        "from transformers import pipeline\n",
        "from diffusers import DiffusionPipeline\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio\n",
        "\n",
        "from diffusers import FluxPipeline\n",
        "\n",
        "import anthropic\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "import google.generativeai\n",
        "\n",
        "import json\n",
        "import gradio as gr\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "import requests\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, AutoModelForSpeechSeq2Seq, AutoProcessor"
      ],
      "metadata": {
        "id": "uT9eqOAWKxwz"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "M19dpvR0KxvK",
        "outputId": "a458d08b-b1ba-438b-c97e-013b21c4ac39"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-654727b7-41d5-4a6d-969d-81fec1e278b1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-654727b7-41d5-4a6d-969d-81fec1e278b1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "anthropic_api_key = os.getenv('CLAUDE_API_KEY')\n",
        "google_api_key = os.getenv('GEMINI_API_KEY')\n",
        "hf_token = os.getenv('HF_TOKEN')\n",
        "\n",
        "check_api_open = f\"OpenAI API Key exists and begins: {openai_api_key[:8]}\" if openai_api_key else \"OpenAI API Key not set\"\n",
        "check_api_anthropic = f\"Anthropic API Key exists and begins: {anthropic_api_key[:7]}\" if anthropic_api_key else \"Anthropic API Key not set\"\n",
        "check_api_gemini = f\"Google API Key exists and begins: {google_api_key[:8]}\" if google_api_key else \"Google API Key not set\"\n",
        "check_api_hugging = f\"Hugging API Key exists and begins: {hf_token[:8]}\" if hf_token else \"Hugging API Key not set\"\n",
        "\n",
        "display(check_api_open, check_api_anthropic, check_api_gemini, check_api_hugging)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "CuZTuPvSKxsf",
        "outputId": "9f8d0588-964d-4f7a-d5ba-fa716e8beda9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'OpenAI API Key exists and begins: sk-proj-'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Anthropic API Key exists and begins: sk-ant-'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Google API Key exists and begins: AIzaSyCH'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Hugging API Key exists and begins: hf_FoxyK'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai = OpenAI(api_key = openai_api_key)\n",
        "claude = anthropic.Anthropic(api_key=anthropic_api_key)\n",
        "google.generativeai.configure(api_key = google_api_key)"
      ],
      "metadata": {
        "id": "hQS9okfuKxqm"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant that reimplements Python code in high performance C++ for an windows. \"\n",
        "system_message += \"Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. \"\n",
        "system_message += \"The C++ response needs to produce an identical output in the fastest possible time. Keep implementations of random number generators identical so that results match exactly.\""
      ],
      "metadata": {
        "id": "Nfp0JGI_Kxnt"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def user_prompt_for(python):\n",
        "    user_prompt = \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
        "    user_prompt += \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
        "    user_prompt += \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
        "    user_prompt += python\n",
        "    return user_prompt\n",
        "\n",
        "\n",
        "def user_prompt_for_testcase(python):\n",
        "    user_prompt = \"Write unit test cases for the C++ code to ensure it produces identical output to the original Python code. \"\n",
        "    user_prompt += \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
        "    user_prompt += \"Include all necessary C++ packages such as iostream, assert, and any other required libraries. \"\n",
        "    user_prompt += \"Ensure that test cases cover a wide range of inputs and edge cases..\\n\\n\"\n",
        "    user_prompt += python\n",
        "    return user_prompt"
      ],
      "metadata": {
        "id": "n9XtJtppKxlQ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def messages_for(python):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
        "    ]\n",
        "\n",
        "def messages_for_testcase(python):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_for_testcase(python)}\n",
        "    ]"
      ],
      "metadata": {
        "id": "LNEqwxcyKxjB"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert python to C++ (more model)**"
      ],
      "metadata": {
        "id": "mqYWsJ4DLJgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT\n",
        "def convert_python_cpp_GPT(python):\n",
        "    stream = openai.chat.completions.create(model=\"gpt-4o\", messages=messages_for(python), stream=True)\n",
        "    reply = \"\"\n",
        "    for chunk in stream:\n",
        "        fragment = chunk.choices[0].delta.content or \"\"\n",
        "        reply += fragment\n",
        "    reply = reply.strip()\n",
        "    reply = reply.replace(\"```cpp\", \"\").replace(\"```\", \"\").strip()\n",
        "    return reply\n",
        "\n",
        "\n",
        "def convert_python_cpp_GPT_testcase(python):\n",
        "    stream = openai.chat.completions.create(model=\"gpt-4o\", messages=messages_for_testcase(python), stream=True)\n",
        "    reply = \"\"\n",
        "    for chunk in stream:\n",
        "        fragment = chunk.choices[0].delta.content or \"\"\n",
        "        reply += fragment\n",
        "    reply = reply.strip()\n",
        "    reply = reply.replace(\"```cpp\", \"\").replace(\"```\", \"\").strip()\n",
        "    return reply"
      ],
      "metadata": {
        "id": "uuXyGbm_KxgW"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Claude\n",
        "def convert_python_cpp_CLAUDE(python):\n",
        "    result = claude.messages.stream(\n",
        "        model=\"claude-3-5-sonnet-20241022\",\n",
        "        max_tokens=2000,\n",
        "        system=system_message,\n",
        "        messages=[{\"role\": \"user\", \"content\": user_prompt_for(python)}],\n",
        "    )\n",
        "    reply = \"\"\n",
        "    with result as stream:\n",
        "        for text in stream.text_stream:\n",
        "            reply += text\n",
        "    reply = reply.strip()\n",
        "    reply = reply.replace(\"```cpp\", \"\").replace(\"```\", \"\").strip()\n",
        "    return reply\n",
        "\n",
        "\n",
        "def convert_python_cpp_CLAUDE_testcase(python):\n",
        "    result = claude.messages.stream(\n",
        "        model=\"claude-3-5-sonnet-20241022\",\n",
        "        max_tokens=2000,\n",
        "        system=system_message,\n",
        "        messages=[{\"role\": \"user\", \"content\": user_prompt_for_testcase(python)}],\n",
        "    )\n",
        "    reply = \"\"\n",
        "    with result as stream:\n",
        "        for text in stream.text_stream:\n",
        "            reply += text\n",
        "    reply = reply.strip()\n",
        "    reply = reply.replace(\"```cpp\", \"\").replace(\"```\", \"\").strip()\n",
        "    return reply"
      ],
      "metadata": {
        "id": "Awf7igxfKxd4"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemini\n",
        "import logging\n",
        "def convert_python_cpp_GEMINI(python_code):\n",
        "    \"\"\"\n",
        "    Chuyển đổi mã Python sang C++ sử dụng mô hình Gemini.\n",
        "\n",
        "    :param python_code: Chuỗi mã Python cần chuyển đổi.\n",
        "    :return: Mã C++ đã chuyển đổi.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.debug(\"Đang kết nối với Gemini...\")\n",
        "        model = google.generativeai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "\n",
        "        logging.debug(\"Đang gửi yêu cầu chuyển đổi mã...\")\n",
        "        start_time = time.time()\n",
        "        response = model.start_chat(\n",
        "            history=[\n",
        "                {\"role\": \"model\", \"parts\": system_message},\n",
        "            ]\n",
        "        ).send_message(user_prompt_for(python_code), stream=True)\n",
        "\n",
        "        reply = \"\"\n",
        "        for chunk in response:\n",
        "            if time.time() - start_time > 60:\n",
        "                return \"Lỗi: Gemini không phản hồi kịp thời.\"\n",
        "            reply += chunk.text\n",
        "\n",
        "        logging.debug(\"Đã nhận phản hồi từ Gemini.\")\n",
        "        if \"```cpp\" in reply and \"```\" in reply:\n",
        "            start_idx = reply.find(\"```cpp\") + len(\"```cpp\")\n",
        "            end_idx = reply.rfind(\"```\")\n",
        "            cpp_code = reply[start_idx:end_idx].strip()\n",
        "        else:\n",
        "            cpp_code = reply.strip()\n",
        "        return cpp_code\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi khi chuyển đổi mã: {e}\")\n",
        "        return f\"Lỗi khi chuyển đổi mã: {e}\"\n",
        "\n",
        "\n",
        "def convert_python_cpp_GEMINI_testcase(python):\n",
        "    model = google.generativeai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "    response = model.start_chat(\n",
        "        history=[\n",
        "            {\"role\": \"model\", \"parts\": system_message},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    response = response.send_message(user_prompt_for_testcase(python), stream=True)\n",
        "    reply = \"\"\n",
        "    for chunk in response:\n",
        "        reply += chunk.text\n",
        "    reply = reply.strip()\n",
        "    reply = reply.replace(\"```cpp\", \"\").replace(\"```\", \"\").strip()\n",
        "    return reply"
      ],
      "metadata": {
        "id": "MrLL_UVuLHgD"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(convert_python_cpp_GPT(\"print('hello world')\"))\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "print(convert_python_cpp_GPT_testcase(\"print('hello world')\"))"
      ],
      "metadata": {
        "id": "yIqTFEWBLHdJ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(convert_python_cpp_CLAUDE(\"print('hello world')\"))\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "print(convert_python_cpp_CLAUDE_testcase(\"print('hello world')\"))"
      ],
      "metadata": {
        "id": "mwJ-SPLiLHbO"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(convert_python_cpp_GEMINI(\"print('hello world')\"))\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "print(convert_python_cpp_GEMINI_testcase(\"print('hello world')\"))"
      ],
      "metadata": {
        "id": "v_7tYFUALHYX"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = google.generativeai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "response = model.generate_content(\"Explain how AI works\", stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.text, end=\"\")"
      ],
      "metadata": {
        "id": "TM9tr2f_LHWV"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = google.generativeai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "chat = model.start_chat(\n",
        "    history=[\n",
        "        {\"role\": \"user\", \"parts\": \"Hello\"},\n",
        "        {\"role\": \"model\", \"parts\": \"Great to meet you. What would you like to know?\"},\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = chat.send_message(\"I have 2 dogs in my house.\", stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.text, end=\"\")"
      ],
      "metadata": {
        "id": "BwweLrWrLHTd"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python_code = \"\"\"\n",
        "# Danh sách các minterm (các tổ hợp đầu vào mà hàm Boolean trả về 1)\n",
        "minterms = [0, 1, 2, 5, 6, 7]\n",
        "\n",
        "# Số biến đầu vào\n",
        "num_variables = 3\n",
        "\n",
        "# Hàm để kiểm tra xem hai số nhị phân có khác nhau đúng 1 bit không\n",
        "def is_adjacent(a, b):\n",
        "    diff = a ^ b\n",
        "    return (diff & (diff - 1)) == 0\n",
        "\n",
        "# Tạo danh sách các nhóm dựa trên số lượng bit 1\n",
        "groups = {}\n",
        "for minterm in minterms:\n",
        "    count = bin(minterm).count('1')\n",
        "    if count not in groups:\n",
        "        groups[count] = []\n",
        "    groups[count].append((minterm, [minterm]))\n",
        "\n",
        "# Tìm các tổ hợp có thể kết hợp\n",
        "prime_implicants = []\n",
        "while True:\n",
        "    new_groups = {}\n",
        "    merged = set()\n",
        "    keys = sorted(groups.keys())\n",
        "    for i in range(len(keys) - 1):\n",
        "        for a in groups[keys[i]]:\n",
        "            for b in groups[keys[i + 1]]:\n",
        "                if is_adjacent(a[0], b[0]):\n",
        "                    merged.add(a[0])\n",
        "                    merged.add(b[0])\n",
        "                    combined = (a[0], a[1] + b[1])\n",
        "                    count = bin(combined[0]).count('1')\n",
        "                    if count not in new_groups:\n",
        "                        new_groups[count] = []\n",
        "                    new_groups[count].append(combined)\n",
        "    for key in groups:\n",
        "        for a in groups[key]:\n",
        "            if a[0] not in merged:\n",
        "                prime_implicants.append(a)\n",
        "    if not new_groups:\n",
        "        break\n",
        "    groups = new_groups\n",
        "\n",
        "# In kết quả (sẽ không hiển thị do stdout bị chuyển hướng)\n",
        "print(\"Các prime implicants:\")\n",
        "for pi in prime_implicants:\n",
        "    print(f\"Minterms: {pi[1]} -> Binary: {format(pi[0], f'0{num_variables}b')}\")\n",
        "\"\"\"\n",
        "\n",
        "result = convert_python_cpp_GEMINI(python_code)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Fu3D3HE8LHQz"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calculate time**"
      ],
      "metadata": {
        "id": "taxI3TykLZU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import sys\n",
        "from io import StringIO\n",
        "import io\n",
        "\n",
        "def execute_python_code(code):\n",
        "    \"\"\"\n",
        "    Thực thi một đoạn mã Python được truyền vào dưới dạng chuỗi.\n",
        "\n",
        "    :param code: Chuỗi mã Python cần thực thi.\n",
        "    :return: Kết quả thực thi, thời gian thực thi và đầu ra của mã.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    old_stdout = sys.stdout\n",
        "    old_stderr = sys.stderr\n",
        "    new_stdout = io.StringIO()\n",
        "    new_stderr = io.StringIO()\n",
        "    sys.stdout = new_stdout\n",
        "    sys.stderr = new_stderr\n",
        "\n",
        "    try:\n",
        "        exec(code)\n",
        "    except Exception as e:\n",
        "        error_message = f\"Execution Error: {e}\\n{new_stderr.getvalue()}\"\n",
        "        return f\"Python Execution Time: {time.time() - start_time:.6f} seconds\", error_message\n",
        "    finally:\n",
        "        sys.stdout = old_stdout\n",
        "        sys.stderr = old_stderr\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    output = new_stdout.getvalue()\n",
        "    error_output = new_stderr.getvalue()\n",
        "\n",
        "    if error_output:\n",
        "        return f\"Python Execution Time: {execution_time:.6f} seconds\"\n",
        "    else:\n",
        "        return f\"Python Execution Time: {execution_time:.6f} seconds\"\n",
        "\n",
        "python_code = \"\"\"\n",
        "# Danh sách các minterm (các tổ hợp đầu vào mà hàm Boolean trả về 1)\n",
        "minterms = [0, 1, 2, 5, 6, 7]\n",
        "\n",
        "# Số biến đầu vào\n",
        "num_variables = 3\n",
        "\n",
        "# Hàm để kiểm tra xem hai số nhị phân có khác nhau đúng 1 bit không\n",
        "def is_adjacent(a, b):\n",
        "    diff = a ^ b\n",
        "    return (diff & (diff - 1)) == 0\n",
        "\n",
        "# Tạo danh sách các nhóm dựa trên số lượng bit 1\n",
        "groups = {}\n",
        "for minterm in minterms:\n",
        "    count = bin(minterm).count('1')\n",
        "    if count not in groups:\n",
        "        groups[count] = []\n",
        "    groups[count].append((minterm, [minterm]))\n",
        "\n",
        "# Tìm các tổ hợp có thể kết hợp\n",
        "prime_implicants = []\n",
        "while True:\n",
        "    new_groups = {}\n",
        "    merged = set()\n",
        "    keys = sorted(groups.keys())\n",
        "    for i in range(len(keys) - 1):\n",
        "        for a in groups[keys[i]]:\n",
        "            for b in groups[keys[i + 1]]:\n",
        "                if is_adjacent(a[0], b[0]):\n",
        "                    merged.add(a[0])\n",
        "                    merged.add(b[0])\n",
        "                    combined = (a[0], a[1] + b[1])\n",
        "                    count = bin(combined[0]).count('1')\n",
        "                    if count not in new_groups:\n",
        "                        new_groups[count] = []\n",
        "                    new_groups[count].append(combined)\n",
        "    for key in groups:\n",
        "        for a in groups[key]:\n",
        "            if a[0] not in merged:\n",
        "                prime_implicants.append(a)\n",
        "    if not new_groups:\n",
        "        break\n",
        "    groups = new_groups\n",
        "\n",
        "# In kết quả (sẽ không hiển thị do stdout bị chuyển hướng)\n",
        "print(\"Các prime implicants:\")\n",
        "for pi in prime_implicants:\n",
        "    print(f\"Minterms: {pi[1]} -> Binary: {format(pi[0], f'0{num_variables}b')}\")\n",
        "\"\"\"\n",
        "\n",
        "result = execute_python_code(python_code)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "xw9Jx4XLLHOD"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_and_run_cpp(code):\n",
        "    \"\"\"\n",
        "    Biên dịch và chạy mã C++.\n",
        "\n",
        "    :param code: Chuỗi mã C++ cần thực thi.\n",
        "    :return: Kết quả thực thi và thời gian thực thi.\n",
        "    \"\"\"\n",
        "\n",
        "    cpp_filename = \"temp.cpp\"\n",
        "    exe_filename = \"temp\"\n",
        "    with open(cpp_filename, \"w\") as f:\n",
        "        f.write(code)\n",
        "\n",
        "    compile_command = f\"g++ {cpp_filename} -o {exe_filename}\"\n",
        "    compile_process = subprocess.run(compile_command, shell=True, capture_output=True, text=True)\n",
        "    if compile_process.returncode != 0:\n",
        "        return f\"Compilation Error: {compile_process.stderr}\"\n",
        "\n",
        "    start_time = time.time()\n",
        "    run_process = subprocess.run(f\"./{exe_filename}\", shell=True, capture_output=True, text=True)\n",
        "    end_time = time.time()\n",
        "\n",
        "    os.remove(cpp_filename)\n",
        "    os.remove(exe_filename)\n",
        "\n",
        "    execution_time = end_time - start_time\n",
        "    return f\"C++ Execution Time: {execution_time} seconds\"\n",
        "\n",
        "\n",
        "cpp_code = convert_python_cpp_GPT(\"print('hello world')\")\n",
        "print(compile_and_run_cpp(cpp_code))"
      ],
      "metadata": {
        "id": "fxpJhlfjLe-h"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_model(python, model_name):\n",
        "    if model_name == \"GPT\":\n",
        "        conversion_function = convert_python_cpp_GPT\n",
        "        test_case_function = convert_python_cpp_GPT_testcase\n",
        "    elif model_name == \"Claude\":\n",
        "        conversion_function = convert_python_cpp_CLAUDE\n",
        "        test_case_function = convert_python_cpp_CLAUDE_testcase\n",
        "    elif model_name == \"Gemini\":\n",
        "        conversion_function = convert_python_cpp_GEMINI\n",
        "        test_case_function = convert_python_cpp_GEMINI_testcase\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model selected\")\n",
        "\n",
        "    try:\n",
        "        cpp_code = conversion_function(python)\n",
        "        unit_test_case = test_case_function(python)\n",
        "        return cpp_code, unit_test_case\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi trong choose_model: {e}\")\n",
        "        return f\"Lỗi: {e}\", \"\""
      ],
      "metadata": {
        "id": "qFpSxDMzLe7o"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "css = \"\"\"\n",
        ".python {background-color: #306998;}\n",
        ".cpp {background-color: #050;}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "krLDApYzLe5e"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(css=css) as ui:\n",
        "    gr.Markdown(\"## Convert code from Python to C++\")\n",
        "    with gr.Row():\n",
        "        python = gr.Textbox(label=\"Python code:\", lines=10)\n",
        "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
        "        test_case = gr.Textbox(label=\"Test case:\", lines=10)\n",
        "    with gr.Row():\n",
        "        model = gr.Dropdown([\"GPT\", \"Claude\", \"Gemini\"], label=\"Select model\", value=\"GPT\")\n",
        "    with gr.Row():\n",
        "        convert = gr.Button(\"Convert code\")\n",
        "    with gr.Row():\n",
        "        python_run = gr.Button(\"Run Python\")\n",
        "        cpp_run = gr.Button(\"Run C++\")\n",
        "    with gr.Row():\n",
        "        python_out = gr.TextArea(label=\"Python result:\", elem_classes=[\"python\"])\n",
        "        cpp_out = gr.TextArea(label=\"C++ result:\", elem_classes=[\"cpp\"])\n",
        "\n",
        "    convert.click(choose_model, inputs=[python, model], outputs=[cpp, test_case])\n",
        "    python_run.click(execute_python_code, inputs=[python], outputs=[python_out])\n",
        "    cpp_run.click(compile_and_run_cpp, inputs=[cpp], outputs=[cpp_out])\n",
        "\n",
        "ui.launch(inbrowser=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "d5QlEutuLe3g",
        "outputId": "7b08f527-f36e-44a4-b7b6-e67e0d5e945e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://67ef62f69fa56b3553.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "48kKCJPaNwHA"
      },
      "execution_count": 67,
      "outputs": []
    }
  ]
}